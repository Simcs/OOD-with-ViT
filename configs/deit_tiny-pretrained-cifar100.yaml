summary: deit_tiny-pretrained-cifar100
model:
  name: deit-tiny
  pretrained: True
  pretrained_model: deit_tiny_patch16_224
  repo: facebookresearch/deit:main
  img_size: 224
  patch_size: 16
  dim_head: 192
  depth: 12
  n_heads: 3
  dim_mlp: 192
  dropout: 0.1
  emb_dropout: 0.1
  layer_name:
    penultimate: fc_norm
    attention: attn_drop

dataset:
  name: CIFAR100
  root: ./data
  n_class: 100
  mean: [0.4914, 0.4822, 0.4465]
  std: [0.2023, 0.1994, 0.2010]

optimizer:
  name: adamw
  base_lr: 0.0001
  weight_decay: 0.0001

scheduler: 
  name: cosine_with_hard_restarts_with_warmup
  warmup_steps: 5
  num_training_steps: 95
  num_cycles: 3
  
train:
  batch_size: 512
  n_epochs: 100
  log_epoch: 10
  use_amp: False
  
eval:
  batch_size: 100