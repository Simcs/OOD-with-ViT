summary: deit_small-pretrained-cifar100
model:
  name: deit-small
  pretrained: True
  pretrained_model: deit_small_patch16_224
  repo: facebookresearch/deit:main
  img_size: 224
  patch_size: 16
  dim_head: 192
  depth: 12
  n_heads: 3
  dim_mlp: 192
  dropout: 0.1
  emb_dropout: 0.1
  layer_name:
    penultimate: pre_logits
    attention: attn_drop

dataset:
  name: CIFAR100
  root: ./data
  n_class: 100
  mean: [0.5071, 0.4867, 0.4408]
  std: [0.2675, 0.2565, 0.2761]

optimizer:
  name: adamw
  base_lr: 0.0001
  weight_decay: 0.0001

scheduler: 
  name: cosine_with_hard_restarts_with_warmup
  warmup_steps: 5
  num_training_steps: 95
  num_cycles: 3
  
train:
  batch_size: 512
  n_epochs: 100
  log_epoch: 10
  use_amp: False
  
eval:
  batch_size: 256