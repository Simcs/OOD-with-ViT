summary: deit_small-pretrained-cifar10
model:
  name: deit-small
  pretrained: True
  pretrained_model: deit_small_patch16_224
  repo: facebookresearch/deit:main
  img_size: 224
  patch_size: 16
  dim_head: 192
  depth: 12
  n_heads: 3
  dim_mlp: 192
  dropout: 0.1
  emb_dropout: 0.1
  layer_name:
    penultimate: pre_logits
    attention: attn_drop

dataset:
  name: CIFAR10
  root: ./data
  n_class: 10
  mean: [0.4914, 0.4822, 0.4465]
  std: [0.2470, 0.2435, 0.2616]

optimizer:
  name: adamw
  base_lr: 0.00005
  weight_decay: 0.0001

scheduler: 
  name: cosine_with_hard_restarts_with_warmup
  warmup_steps: 5
  num_training_steps: 95
  num_cycles: 3

train:
  batch_size: 512
  n_epochs: 100
  log_epoch: 10
  use_amp: False

eval:
  batch_size: 256