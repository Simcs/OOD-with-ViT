summary: deit_tiny-pretrained-cifar10
model:
  name: deit-tiny
  pretrained: True
  pretrained_model: deit_tiny_patch16_224
  repo: facebookresearch/deit:main
  img_size: 224
  patch_size: 16
  dim_head: 192
  depth: 12
  n_heads: 3
  dim_mlp: 192
  dropout: 0.1
  emb_dropout: 0.1
  layer_name:
    penultimate: pre_logits
    attention: attn_drop

dataset:
  name: CIFAR10
  root: ./data
  n_class: 10
  mean: [0.4914, 0.4822, 0.4465]
  std: [0.2023, 0.1994, 0.2010]

optimizer:
  name: adamw
  base_lr: 0.00005
  weight_decay: 0.0001

scheduler: 
  name: cosine

train:
  batch_size: 512
  n_epochs: 100
  log_epoch: 10
  use_amp: False

eval:
  batch_size: 100