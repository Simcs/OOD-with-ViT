{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from IPython.display import display\n",
    "\n",
    "import video_transformer.data_transform as T\n",
    "from video_transformer.dataset import DecordInit, load_annotation_data\n",
    "from video_transformer.transformer import PatchEmbed, TransformerContainer, ClassificationHead\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare kinetics400 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/kinetics400_val_metadata.pkl', 'rb') as f:\n",
    "    kinetics400_val_metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/simc/workspace/dataset/kinetics/k400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/datasets/video_utils.py:215: UserWarning: There aren't enough frames in the current video to get a clip for the given clip length and frames between clips. The video (and potentially others) will be skipped.\n",
      "  warnings.warn(\"There aren't enough frames in the current video to get a clip for the given clip length and \"\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import Kinetics\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "dataset_root = '~/workspace/dataset/kinetics/k400'\n",
    "dataset_root = os.path.expanduser(dataset_root)\n",
    "print(dataset_root)\n",
    "\n",
    "dataset_mean, dataset_std = (0.45, 0.45, 0.45), (0.225, 0.225, 0.225)\n",
    "val_transform = T.create_video_transform(\n",
    "    input_size=224,\n",
    "    is_training=False,\n",
    "    interpolation='bicubic',\n",
    "    mean=dataset_mean,\n",
    "    std=dataset_std,\n",
    ")\n",
    "data_transform = T.Compose([\n",
    "    T.Resize(scale_range=(-1, 256)),\n",
    "    T.ThreeCrop(size=224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(dataset_mean, dataset_std)\n",
    "])\n",
    "data_transform.randomize_parameters()\n",
    "\n",
    "kinetics400_val_ds = Kinetics(\n",
    "    root=dataset_root,\n",
    "    frames_per_clip=16,\n",
    "    split='val',\n",
    "    num_workers=16,\n",
    "    frame_rate=2,\n",
    "    step_between_clips=2,\n",
    "    transform=val_transform,\n",
    "    _precomputed_metadata=kinetics400_val_metadata,\n",
    ")\n",
    "\n",
    "kinetics400_val_dl = DataLoader(\n",
    "    dataset=kinetics400_val_ds,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video paths: <class 'list'> 19881 /home/simc/workspace/dataset/kinetics/k400/val/abseiling/0wR5jVB-WPk_000417_000427.mp4\n",
      "video pts: <class 'list'> 19881 torch.Size([300])\n",
      "video fps: <class 'list'> 19881 29.97002997002997\n",
      "clips <class 'list'> 19881 torch.Size([3, 16])\n",
      "cumulative sizes: 53144\n"
     ]
    }
   ],
   "source": [
    "video_paths = kinetics400_val_ds.metadata['video_paths']\n",
    "video_pts = kinetics400_val_ds.metadata['video_pts']\n",
    "video_fps = kinetics400_val_ds.metadata['video_fps']\n",
    "clips = kinetics400_val_ds.video_clips.clips\n",
    "print('video paths:', type(video_paths), len(video_paths), video_paths[0])\n",
    "print('video pts:', type(video_pts), len(video_pts), video_pts[0].shape)\n",
    "print('video fps:', type(video_fps), len(video_fps), video_fps[0])\n",
    "print('clips', type(clips), len(clips), clips[0].shape)\n",
    "print('cumulative sizes:', kinetics400_val_ds.video_clips.cumulative_sizes[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to restart the Kernel. \n",
      "Unable to start Kernel 'Python 3.9.7 64-bit' due to connection timeout. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for data in kinetics400_val_ds:\n",
    "    video, audio, label = data\n",
    "    print(video.shape, label, kinetics400_val_ds.classes[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 56, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [2, 329728] at entry 0 and [2, 331396] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2272293/1256078870.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkinetics400_val_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 56, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [2, 329728] at entry 0 and [2, 331396] at entry 1\n"
     ]
    }
   ],
   "source": [
    "for x, y in kinetics400_val_dl:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare ViViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_transformer.video_transformer import ViViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_state_dict(state_dict):\n",
    "\tfor old_key in list(state_dict.keys()):\n",
    "\t\tif old_key.startswith('model'):\n",
    "\t\t\tnew_key = old_key[6:]\n",
    "\t\t\tstate_dict[new_key] = state_dict.pop(old_key)\n",
    "\t\telse:\n",
    "\t\t\tnew_key = old_key[9:]\n",
    "\t\t\tstate_dict[new_key] = state_dict.pop(old_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_from_pretrain_(module, pretrained, init_module):\n",
    "    if torch.cuda.is_available():\n",
    "        state_dict = torch.load(pretrained)\n",
    "    else:\n",
    "        state_dict = torch.load(pretrained, map_location=torch.device('cpu'))\n",
    "    if init_module == 'transformer':\n",
    "        replace_state_dict(state_dict)\n",
    "    elif init_module == 'cls_head':\n",
    "        replace_state_dict(state_dict)\n",
    "    else:\n",
    "        raise TypeError(f'pretrained weights do not include the {init_module} module')\n",
    "    msg = module.load_state_dict(state_dict, strict=False)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['cls_head.weight', 'cls_head.bias'])\n",
      "load model finished, the missing key of cls is:[]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "num_frames = 8\n",
    "frame_interval = 32\n",
    "num_class = 400\n",
    "arch = 'vivit' # turn to vivit for initializing vivit model\n",
    "\n",
    "pretrain_pth = './logs/vivit/vivit_model.pth'\n",
    "num_frames = num_frames * 2\n",
    "frame_interval = frame_interval // 2\n",
    "model = ViViT(\n",
    "    num_frames=num_frames,\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    embed_dims=768,\n",
    "    in_channels=3,\n",
    "    attention_type='fact_encoder',\n",
    "    return_cls_token=True,\n",
    "    pretrain_pth=pretrain_pth,\n",
    "    weights_from='kinetics',\n",
    ")\n",
    "\n",
    "cls_head = ClassificationHead(num_classes=num_class, in_channels=768)\n",
    "# msg_trans = init_from_pretrain_(model, pretrain_pth, init_module='transformer')\n",
    "msg_cls = init_from_pretrain_(cls_head, pretrain_pth, init_module='cls_head')\n",
    "model.eval()\n",
    "cls_head.eval()\n",
    "model = model.to(device)\n",
    "cls_head = cls_head.to(device)\n",
    "print(f'load model finished, the missing key of cls is:{msg_cls[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"480\" height=\"480\" src=\"./YABnJL_bDzw.mp4\">animation</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "video_path = './YABnJL_bDzw.mp4'\n",
    "html_str = '''\n",
    "<video controls width=\\\"480\\\" height=\\\"480\\\" src=\\\"{}\\\">animation</video>\n",
    "'''.format(video_path)\n",
    "display(HTML(html_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302 22 278\n",
      "original: (16, 256, 454, 3)\n",
      "transformed: torch.Size([3, 16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Prepare data preprocess\n",
    "mean, std = (0.45, 0.45, 0.45), (0.225, 0.225, 0.225)\n",
    "data_transform = T.Compose([\n",
    "        T.Resize(scale_range=(-1, 256)),\n",
    "        T.ThreeCrop(size=224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std)\n",
    "        ])\n",
    "temporal_sample = T.TemporalRandomCrop(num_frames*frame_interval)\n",
    "\n",
    "# Sampling video frames\n",
    "video_decoder = DecordInit()\n",
    "v_reader = video_decoder(video_path)\n",
    "total_frames = len(v_reader)\n",
    "start_frame_ind, end_frame_ind = temporal_sample(total_frames)\n",
    "print(total_frames, start_frame_ind, end_frame_ind)\n",
    "if end_frame_ind-start_frame_ind < num_frames:\n",
    "    raise ValueError(f'the total frames of the video {video_path} is less than {num_frames}')\n",
    "frame_indice = np.linspace(0, end_frame_ind-start_frame_ind-1, num_frames, dtype=int)\n",
    "video = v_reader.get_batch(frame_indice).asnumpy()\n",
    "del v_reader\n",
    "\n",
    "print('original:', video.shape)\n",
    "video = torch.from_numpy(video).permute(0,3,1,2) # Video transform: T C H W\n",
    "data_transform.randomize_parameters()\n",
    "video = data_transform(video)\n",
    "video = video.to(device)\n",
    "print('transformed:', video.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 400])\n",
      "the shape of ouptut: torch.Size([400]),     and the prediction is: laughing\n"
     ]
    }
   ],
   "source": [
    "# Predict class label\n",
    "with torch.no_grad():\n",
    "    logits = model(video)\n",
    "    output = cls_head(logits)\n",
    "    print(output.shape)\n",
    "    output = output.view(3, 400).mean(0)\n",
    "    cls_pred = output.argmax().item()\n",
    "    \n",
    "print(f'the shape of ouptut: {output.shape}, \\\n",
    "    and the prediction is: {kinetics400_val_ds.classes[cls_pred]}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
